{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import openpyxl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========> Notation <=========\n",
      "===> [o] - Success\n",
      "===> [x] - Error\n",
      "===> [i] - Ignored\n",
      "===> [n] - synset not found\n",
      "==============================\n",
      "\n",
      "Crwling word range add here (int vals) [end = -1 means till end of data]\n",
      "Start of range:500000\n",
      "End of range:1000000\n",
      "[o] Data loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('=========> Notation <=========')\n",
    "print('===> [o] - Success')\n",
    "print('===> [x] - Error')\n",
    "print('===> [i] - Ignored')\n",
    "print('===> [n] - Synset not found')\n",
    "print('==============================\\n')\n",
    "\n",
    "#Get range for crawler\n",
    "print(\"Crwling word range add here (int vals) [end = -1 means till end of data]\")\n",
    "start = int(input(\"Start of range:\"))\n",
    "end = int(input(\"End of range:\"))\n",
    "\n",
    "#read fasttext vector\n",
    "f = open(\"FastText/cc.en.300.vec\",'r', encoding='utf-8')\n",
    "data = f.readlines()\n",
    "print('[o] Data loaded Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_frame(word):     \n",
    "    url = 'https://www.maduraonline.com/?find='+word\n",
    "    \n",
    "    header = {\n",
    "      \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "      \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url, headers=header)\n",
    "\n",
    "    dfs = pd.read_html(r.text)[0]\n",
    "    if(len(dfs.columns)>1):\n",
    "        dfs.drop(columns=[0],inplace=True)\n",
    "        dfs.dropna(inplace=True)\n",
    "        dfs[word] =dfs[2].apply(lambda x: x.replace(\" \",\"_\")) #dfs[word] = dfs[1]+dfs[2]\n",
    "        dfs.drop(columns=[1,2],inplace=True)\n",
    "        return dfs.T\n",
    "    else:\n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====> Crawling Starts <=====\n",
      "\n",
      "[500000][n] | Word: Chiarini\n",
      "[500001][n] | Word: prenomen\n",
      "[500002][i] | Word: Palestine-related\n",
      "[500003][n] | Word: NGFW\n",
      "[500004][n] | Word: Thielmann\n",
      "[500005][i] | Word: interpretation.\n",
      "[500006][n] | Word: Olicity\n",
      "[500007][n] | Word: Bernardston\n",
      "[500008][i] | Word: Jean-René\n",
      "[500009][n] | Word: NewsReviewPhonesGiveawayDealsShop\n",
      "[500010][n] | Word: AgainYesterday\n",
      "[500011][n] | Word: Dormy\n",
      "[500012][n] | Word: Bardfield\n",
      "[500013][i] | Word: km1500\n",
      "[500014][i] | Word: full-mouth\n",
      "[500015][n] | Word: Palmwoods\n",
      "[500016][n] | Word: PONTE\n",
      "[500017][n] | Word: Glans\n",
      "[500018][i] | Word: visit.by\n",
      "[500019][i] | Word: single-layered\n",
      "[500020][n] | Word: spoilerish\n",
      "[500021][n] | Word: DealersJunk\n",
      "[500022][n] | Word: eved\n",
      "[500023][n] | Word: WELLBreakfastLunchDinnerHealthy\n",
      "[500024][n] | Word: Meminger\n",
      "[500025][n] | Word: Heejun\n",
      "[500026][n] | Word: Videx\n",
      "[500027][i] | Word: OB1\n",
      "[500028][n] | Word: goodApr\n",
      "[500029][i] | Word: -many\n",
      "[500030][n] | Word: presstitutes\n",
      "[500031][o] | Word: CHEATER\n",
      "[500032][i] | Word: 4363\n",
      "[500033][n] | Word: Ballymahon\n",
      "[500034][n] | Word: choas\n",
      "[500035][n] | Word: threatend\n",
      "[500036][i] | Word: 2,481\n",
      "[500037][n] | Word: Sharrett\n",
      "[500038][n] | Word: imf\n",
      "[500039][n] | Word: Christene\n",
      "[500040][n] | Word: Luchi\n",
      "[500041][n] | Word: Cleere\n",
      "[500042][n] | Word: Vivos\n",
      "[500043][i] | Word: death.He\n",
      "[500044][n] | Word: Beinhorn\n",
      "[500045][n] | Word: Harrill\n",
      "[500046][n] | Word: bathroomFurnishingUnfurnishedAvailableNowLength\n",
      "[500047][n] | Word: luffa\n",
      "[500048][i] | Word: Saint-Barthélemy\n",
      "[500049][n] | Word: CommentBy\n",
      "[500050][i] | Word: Targets263\n",
      "[500051][n] | Word: drolly\n",
      "[500052][n] | Word: SmartTrack\n",
      "[500053][n] | Word: Akosombo\n",
      "[500054][i] | Word: Dereks1x\n",
      "[500055][n] | Word: Renuzit\n",
      "[500056][i] | Word: PenginapanApartmenMemuatkan1Bilik\n",
      "[500057][n] | Word: handstitching\n",
      "[500058][n] | Word: patte\n",
      "[500059][n] | Word: Keidel\n",
      "[500060][i] | Word: Y.a.s.\n",
      "[500061][n] | Word: Fionnula\n",
      "[500062][n] | Word: likeLifehackerAll\n",
      "[500063][i] | Word: 50.82\n",
      "[500064][n] | Word: Somm\n",
      "[500065][i] | Word: -except\n",
      "[500066][n] | Word: Madonie\n",
      "[500067][i] | Word: 40-40\n",
      "[500068][n] | Word: arbitrageurs\n",
      "[500069][n] | Word: Owa\n",
      "[500070][i] | Word: .State\n",
      "[500071][n] | Word: Wurzels\n",
      "[500072][n] | Word: Wainui\n",
      "[500073][n] | Word: stationeries\n",
      "[500074][n] | Word: Lirico\n",
      "[500075][n] | Word: Tismăneanu\n",
      "[500076][n] | Word: Sharpham\n",
      "[500077][i] | Word: C-X75\n",
      "[500078][i] | Word: 4-AP\n",
      "[500079][n] | Word: Malk\n",
      "[500080][n] | Word: Pleae\n",
      "[500081][n] | Word: SHOWERS\n",
      "[500082][i] | Word: 3,145\n",
      "[500083][i] | Word: NY-23\n",
      "[500084][n] | Word: Ashuelot\n",
      "[500085][n] | Word: bellbottoms\n",
      "[500086][n] | Word: Pillowtalk\n",
      "[500087][n] | Word: gesehen\n",
      "[500088][n] | Word: honker\n",
      "[500089][n] | Word: brainbuster\n",
      "[500090][n] | Word: furthe\n",
      "[500091][i] | Word: Screw-in\n",
      "[500092][n] | Word: Partyka\n",
      "[500093][n] | Word: Fenson\n",
      "[500094][n] | Word: Forthright\n",
      "[500095][n] | Word: Geewax\n",
      "[500096][n] | Word: Jayhawkers\n",
      "[500097][n] | Word: smurfy\n",
      "[500098][n] | Word: MWRA\n",
      "[500099][n] | Word: BlackMagic\n",
      "[500100][n] | Word: CJF\n",
      "[500101][n] | Word: IdeaTab\n",
      "[500102][n] | Word: aew\n",
      "[500103][i] | Word: 3,430\n",
      "[500104][i] | Word: forty-\n",
      "[500105][i] | Word: West-East\n",
      "[500106][n] | Word: Puedes\n",
      "[500107][n] | Word: dTTP\n",
      "[500108][i] | Word: Fixed-term\n",
      "[500109][n] | Word: Laranjeiras\n",
      "[500110][n] | Word: Jizzy\n",
      "[500111][i] | Word: two-byte\n",
      "[500112][n] | Word: zaniest\n",
      "[500113][n] | Word: veloute\n",
      "[500114][n] | Word: Jahaan\n",
      "[500115][i] | Word: receptors.Medicine\n",
      "[500116][n] | Word: Grindstaff\n",
      "[500117][n] | Word: Krusenstern\n",
      "[500118][n] | Word: IJL\n",
      "[500119][i] | Word: summary.html\n",
      "[500120][i] | Word: 21.01\n",
      "[500121][i] | Word: Review.\n",
      "[500122][n] | Word: umgeben\n",
      "[500123][n] | Word: ManMay\n",
      "[500124][n] | Word: Verdone\n",
      "[500125][n] | Word: Bolsinger\n",
      "[500126][i] | Word: 608.\n",
      "[500127][n] | Word: Raanjhanaa\n",
      "[500128][n] | Word: Irna\n",
      "[500129][i] | Word: Backup4all\n",
      "[500130][i] | Word: Cir.1977\n",
      "[500131][i] | Word: 20Media\n",
      "[500132][n] | Word: Maslenitsa\n",
      "[500133][o] | Word: MoB\n",
      "[500134][i] | Word: .Imagine\n",
      "[500135][n] | Word: spiracle\n",
      "[500136][n] | Word: telic\n",
      "[500137][i] | Word: free-air\n",
      "[500138][n] | Word: HLTH\n",
      "[500139][n] | Word: technophobes\n",
      "[500140][i] | Word: Spot-on\n",
      "[500141][n] | Word: Zelenograd\n",
      "[500142][i] | Word: 1944-1946\n",
      "[500143][o] | Word: BIll\n",
      "[500144][n] | Word: deworm\n",
      "[500145][i] | Word: www.ajc.com\n",
      "[500146][i] | Word: Seattle.\n",
      "[500147][i] | Word: --brian0918\n",
      "[500148][n] | Word: Wiscon\n",
      "[500149][n] | Word: Kodra\n",
      "[500150][n] | Word: etymologist\n",
      "[500151][n] | Word: wilbur\n",
      "[500152][n] | Word: Mulkern\n",
      "[500153][n] | Word: Donepezil\n",
      "[500154][i] | Word: NEWS18.com\n",
      "[500155][i] | Word: 7What\n",
      "[500156][n] | Word: Proietti\n",
      "[500157][i] | Word: reviewsDistance3.8\n",
      "[500158][n] | Word: LOLClick\n",
      "[500159][n] | Word: Kazbegi\n",
      "[500160][n] | Word: Jinki\n",
      "[500161][n] | Word: Nemirovsky\n",
      "[500162][n] | Word: delted\n",
      "[500163][i] | Word: rights.In\n",
      "[500164][n] | Word: Dreadlock\n",
      "[500165][n] | Word: Vanderburg\n",
      "[500166][n] | Word: Ndoye\n",
      "[500167][n] | Word: couldhave\n",
      "[500168][n] | Word: Wambui\n",
      "[500169][n] | Word: Milto\n",
      "[500170][i] | Word: G2a\n"
     ]
    }
   ],
   "source": [
    "total_vector_count = 0\n",
    "new_vecs = ''\n",
    "\n",
    "#check user input crawl end\n",
    "if(end==-1 or end>len(data)):\n",
    "    end = len(data)\n",
    "\n",
    "print(\"\\n=====> Crawling Starts <=====\\n\")\n",
    "for i in range(start,end):\n",
    "    try:\n",
    "        word_vector = data[i].split(\" \", 1)\n",
    "        if(not word_vector[0].isalpha()):\n",
    "            print(\"[\"+str(i)+\"][i] | Word: \"+word_vector[0])\n",
    "            continue\n",
    "        si_words = html_to_frame(word_vector[0])\n",
    "        si_words_count = len(si_words.columns)\n",
    "        if(si_words_count>1):\n",
    "            print(\"[\"+str(i)+\"][o] | Word: \"+word_vector[0])\n",
    "            temp_vec = ''\n",
    "            for i in list(dict.fromkeys(si_words.to_string(header=False,index=False).split())):\n",
    "                total_vector_count +=1 \n",
    "                temp_vec+= (i+\" \"+word_vector[1])      \n",
    "            new_vecs+= temp_vec\n",
    "        else:\n",
    "            print(\"[\"+str(i)+\"][n] | Word: \"+word_vector[0])\n",
    "            continue\n",
    "    except:\n",
    "        print(\"[\"+str(i)+\"][x] | Word: \"+str(word_vector[0]))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the vec file\n",
    "to_write = str(total_vector_count)+\" \"+data[0].split()[1]+'\\n'+new_vecs\n",
    "writer = open(\"Output/tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec\", \"w\", encoding=\"utf-8\")\n",
    "writer.write(to_write)\n",
    "writer.close()\n",
    "\n",
    "#Write new models to combine script\n",
    "f=open(\"Output/new_models.txt\", \"a+\")\n",
    "f.write(\"tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec\\n\")\n",
    "f.close()\n",
    "\n",
    "#final\n",
    "print(\"====> Done <====\")\n",
    "print(\"[o] Total new vectors:\",total_vector_count)\n",
    "print(\"[o] Vecors saved to 'tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec' file successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
