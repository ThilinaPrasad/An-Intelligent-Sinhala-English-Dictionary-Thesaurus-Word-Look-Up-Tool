{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import openpyxl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========> Notation <=========\n",
      "===> [o] - Success\n",
      "===> [x] - Error\n",
      "===> [i] - Ignored\n",
      "==============================\n",
      "\n",
      "Crwling word range add here (int vals)\n",
      "Start of range:100\n",
      "End of range:200\n",
      "[o] Data loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('=========> Notation <=========')\n",
    "print('===> [o] - Success')\n",
    "print('===> [x] - Error')\n",
    "print('===> [i] - Ignored')\n",
    "print('==============================\\n')\n",
    "\n",
    "#Get range for crawler\n",
    "print(\"Crwling word range add here (int vals) [end = -1 means till end of data]\")\n",
    "start = int(input(\"Start of range:\"))\n",
    "end = int(input(\"End of range:\"))\n",
    "\n",
    "#read fasttext vector\n",
    "f = open(\"FastText/cc.en.300.vec\",'r', encoding='utf-8')\n",
    "data = f.readlines()\n",
    "print('[o] Data loaded Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_frame(word):     \n",
    "    url = 'https://www.maduraonline.com/?find='+word\n",
    "    \n",
    "    header = {\n",
    "      \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "      \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url, headers=header)\n",
    "\n",
    "    dfs = pd.read_html(r.text)[0]\n",
    "    if(len(dfs.columns)>1):\n",
    "        dfs.drop(columns=[0],inplace=True)\n",
    "        dfs.dropna(inplace=True)\n",
    "        dfs[word] =dfs[2].apply(lambda x: x.replace(\" \",\"_\")) #dfs[word] = dfs[1]+dfs[2]\n",
    "        dfs.drop(columns=[1,2],inplace=True)\n",
    "        return dfs.T\n",
    "    else:\n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100][o] | Word: only\n",
      "[101][o] | Word: first\n",
      "[102][o] | Word: any\n",
      "[103][o] | Word: its\n",
      "[104][o] | Word: people\n",
      "[105][i] | Word: 2\n",
      "[106][i] | Word: $\n",
      "[107][o] | Word: very\n",
      "[108][o] | Word: t\n",
      "[109][o] | Word: over\n",
      "[110][o] | Word: she\n",
      "[111][i] | Word: %\n",
      "[112][o] | Word: how\n",
      "[113][o] | Word: make\n",
      "[114][o] | Word: You\n",
      "[115][o] | Word: said\n",
      "[116][o] | Word: He\n",
      "[117][o] | Word: two\n",
      "[118][o] | Word: may\n",
      "[119][o] | Word: know\n",
      "[120][o] | Word: then\n",
      "[121][o] | Word: see\n",
      "[122][o] | Word: after\n",
      "[123][o] | Word: most\n",
      "[124][o] | Word: good\n",
      "[125][o] | Word: years\n",
      "[126][o] | Word: If\n",
      "[127][o] | Word: these\n",
      "[128][o] | Word: now\n",
      "[129][i] | Word: 3\n",
      "[130][o] | Word: use\n",
      "[131][o] | Word: because\n",
      "[132][o] | Word: well\n",
      "[133][o] | Word: work\n",
      "[134][o] | Word: could\n",
      "[135][o] | Word: us\n",
      "[136][o] | Word: don\n",
      "[137][o] | Word: way\n",
      "[138][o] | Word: much\n",
      "[139][o] | Word: back\n",
      "[140][o] | Word: many\n",
      "[141][o] | Word: think\n",
      "[142][o] | Word: where\n",
      "[143][o] | Word: even\n",
      "[144][o] | Word: him\n",
      "[145][o] | Word: through\n",
      "[146][o] | Word: am\n",
      "[147][i] | Word: 10\n",
      "[148][i] | Word: |\n",
      "[149][o] | Word: here\n",
      "[150][i] | Word: #\n",
      "[151][o] | Word: made\n",
      "[152][o] | Word: year\n",
      "[153][o] | Word: should\n",
      "[154][i] | Word: *\n",
      "[155][o] | Word: really\n",
      "[156][o] | Word: being\n",
      "[157][o] | Word: such\n",
      "[158][o] | Word: need\n",
      "[159][o] | Word: great\n",
      "[160][o] | Word: And\n",
      "[161][i] | Word: ]\n",
      "[162][i] | Word: 4\n",
      "[163][i] | Word: [\n",
      "[164][i] | Word: 5\n",
      "[165][o] | Word: day\n",
      "[166][o] | Word: before\n",
      "[167][o] | Word: want\n",
      "[168][o] | Word: used\n",
      "[169][o] | Word: go\n",
      "[170][o] | Word: those\n",
      "[171][i] | Word: …\n",
      "[172][o] | Word: But\n",
      "[173][o] | Word: right\n",
      "[174][i] | Word: 'm\n",
      "[175][o] | Word: take\n",
      "[176][i] | Word: —\n",
      "[177][o] | Word: May\n",
      "[178][o] | Word: still\n",
      "[179][o] | Word: last\n",
      "[180][o] | Word: off\n",
      "[181][o] | Word: too\n",
      "[182][o] | Word: New\n",
      "[183][o] | Word: going\n",
      "[184][o] | Word: best\n",
      "[185][o] | Word: find\n",
      "[186][o] | Word: love\n",
      "[187][o] | Word: did\n",
      "[188][o] | Word: while\n",
      "[189][o] | Word: home\n",
      "[190][o] | Word: There\n",
      "[191][o] | Word: They\n",
      "[192][o] | Word: same\n",
      "[193][o] | Word: around\n",
      "[194][o] | Word: help\n",
      "[195][o] | Word: down\n",
      "[196][o] | Word: information\n",
      "[197][o] | Word: UTC\n",
      "[198][o] | Word: place\n",
      "[199][o] | Word: i\n"
     ]
    }
   ],
   "source": [
    "total_vector_count = 0\n",
    "new_vecs = ''\n",
    "if(end==-1):\n",
    "    end = len(data)\n",
    "for i in range(start,end):\n",
    "    try:\n",
    "        word_vector = data[i].split(\" \", 1)\n",
    "        if(not word_vector[0].isalpha()):\n",
    "            print(\"[\"+str(i)+\"][i] | Word: \"+word_vector[0])\n",
    "            continue\n",
    "        print(\"[\"+str(i)+\"][o] | Word: \"+word_vector[0])\n",
    "        si_words = html_to_frame(word_vector[0])\n",
    "        si_words_count = len(si_words.columns)\n",
    "        if(si_words_count>1):\n",
    "            temp_vec = ''\n",
    "            for i in list(dict.fromkeys(si_words.to_string(header=False,index=False).split())):\n",
    "                total_vector_count +=1 \n",
    "                temp_vec+= (i+\" \"+word_vector[1])      \n",
    "            new_vecs+= temp_vec\n",
    "        else:\n",
    "            continue\n",
    "    except:\n",
    "        print(\"[\"+str(i)+\"][x] | Word: \"+str(word_vector[0]))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Done <====\n",
      "[o] Total new vectors: 948\n",
      "[o] Vecors saved to 'tr.cc.si.300_100_200.vec' file successfully.\n"
     ]
    }
   ],
   "source": [
    "#Write the vec file\n",
    "to_write = str(total_vector_count)+\" \"+data[0].split()[1]+'\\n'+new_vecs\n",
    "writer = open(\"Output/tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec\", \"w\", encoding=\"utf-8\")\n",
    "writer.write(to_write)\n",
    "writer.close()\n",
    "\n",
    "#Write new models to combine script\n",
    "f=open(\"Output/new_models.txt\", \"a+\")\n",
    "f.write(\"tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec\\n\")\n",
    "f.close()\n",
    "\n",
    "#final\n",
    "print(\"====> Done <====\")\n",
    "print(\"[o] Total new vectors:\",total_vector_count)\n",
    "print(\"[o] Vecors saved to 'tr.cc.si.300_\"+str(start)+\"_\"+str(end)+\".vec' file successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
